@article{quentin2021,
   abstract = {In navigation, deep learning for inertial odometry (IO) has recently been investigated using data from a low-cost IMU only. The measurement of noise, bias, and some errors from which IO suffers is estimated with a deep neural network (DNN) to achieve more accurate pose estimation. While numerous studies on the subject highlighted the performances of their approach, the behavior of data-driven IO with DNN has not been clarified. Therefore, this paper presents a quantitative analysis of kinematics-mimicking DNN-based IO from various aspects. First, the new network architecture is designed to mimic the kinematics and ensure comprehensive analyses. Next, the hyper-parameters of neural networks that are highly correlated to IO are identified. Besides, their role in the performances is investigated. In the evaluation, the analyses were conducted with publicly-available IO datasets for vehicles and drones. The results are introduced to highlight the remaining problems in IO and are considered a guideline to promote further research.},
   author = {Quentin Arnaud Dugne-Hennequin and Hideaki Uchiyama and Joao Paulo Silva Do Monte Lima},
   doi = {10.1109/ACCESS.2021.3062817},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {IMU,Inertial odometry,dead reckoning,deep neural network,kinematics,navigation},
   pages = {36589-36619},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Understanding the Behavior of Data-Driven Inertial Odometry with Kinematics-Mimicking Deep Neural Network},
   volume = {9},
   year = {2021},
}

@article{chen2022learning,
   abstract = {Autonomous vehicles and mobile robotic systems are typically equipped with multiple sensors to provide redundancy. By integrating the observations from different sensors, these mobile agents are able to perceive the environment and estimate system states, e.g., locations and orientations. Although deep learning (DL) approaches for multimodal odometry estimation and localization have gained traction, they rarely focus on the issue of robust sensor fusion--a necessary consideration to deal with noisy or incomplete sensor observations in the real world. Moreover, current deep odometry models suffer from a lack of interpretability. To this extent, we propose SelectFusion, an end-to-end selective sensor fusion module that can be applied to useful pairs of sensor modalities, such as monocular images and inertial measurements, depth images, and light detection and ranging (LIDAR) point clouds. Our model is a uniform framework that is not restricted to specific modality or task. During prediction, the network is able to assess the reliability of the latent features from different sensor modalities and to estimate trajectory at both scale and global pose. In particular, we propose two fusion modules--a deterministic soft fusion and a stochastic hard fusion--and offer a comprehensive study of the new strategies compared with trivial direct fusion. We extensively evaluate all fusion strategies both on public datasets and on progressively degraded datasets that present synthetic occlusions, noisy and missing data, and time misalignment between sensors, and we investigate the effectiveness of the different fusion strategies in attending the most reliable features, which in itself provides insights into the operation of the various models.},
   author = {Changhao Chen and Stefano Rosa and Chris Xiaoxuan Lu and Bing Wang and Niki Trigoni and Andrew Markham},
   doi = {10.1109/TNNLS.2022.3176677},
   issn = {21622388},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep neural networks (DNNs),Feature extraction,Laser radar,Location awareness,Robot sensing systems,Sensor fusion,Task analysis,Visualization,feature selection,localization,multimodal learning,point cloud odometry,robot navigation,sensor fusion,visual-inertial odometry (VIO).},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Learning Selective Sensor Fusion for State Estimation},
   year = {2022},
}

@article{chen2022contrastive,
   abstract = {Robust and accurate zero-velocity detection can improve the performance of zero-velocity-aided foot-mounted inertial navigation system. To ensure the accuracy of zero-velocity detection, we propose a novel detector based on contrastive learning. This detector roughly eliminates the inertial data that must not be the zero-velocity event in advance, to reduce the computation cost. Then the detector uses the remaining inertial data to detect the zero-velocity event via a trained contrastive neural network. The contrastive neural network uses the triplet network and is trained by comparing with the anchor data which consists of the known static inertial data from the period of initial alignment. The classifier will finally determine whether the output of the triplet network is the zero-velocity event. Two experiments were conducted to evaluate this novel detector, showing that it can adaptively and accurately detect the zero-velocity event. The horizontal position errors of the two experiments are respectively 1.33m over a 953m outdoor path with walking and 3.74m over a 1143m indoor/outdoor path with combined motion of low dynamic and high dynamic.},
   author = {Ze Chen and Xianfei Pan and Changhao Chen and Meiping Wu},
   doi = {10.1109/JSEN.2021.3072160},
   issn = {1530-437X},
   issue = {6},
   journal = {IEEE Sensors Journal},
   keywords = {Pedestrian navigation,contrastive learning,inertial sensors,triplet network,zero-velocity detection},
   month = {3},
   pages = {4962-4969},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Contrastive Learning of Zero-Velocity Detection for Pedestrian Inertial Navigation},
   volume = {22},
   url = {https://ieeexplore.ieee.org/document/9399505/},
   year = {2022},
}

@article{chen2021dynanet,
   abstract = {Dynamical models estimate and predict the temporal evolution of physical systems. State-space models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations, e.g., the Kalman filter. However, they require significant domain knowledge to derive the parametric form and considerable hand tuning to correctly set all the parameters. Data-driven techniques, e.g., recurrent neural networks, have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their impressive capability to extract relevant features from rich inputs. They, however, lack interpretability and robustness to unseen conditions. Thus, data-driven models are hard to be applied in safety-critical applications, such as self-driving vehicles. In this work, we present DynaNet, a hybrid deep learning and time-varying SSM, which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of both SSM and deep neural networks. We demonstrate its effectiveness in the estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation, and motion prediction. In addition, we show how DynaNet can indicate failures through investigation of properties, such as the rate of innovation (Kalman gain).},
   author = {Changhao Chen and Chris Xiaoxuan Lu and Bing Wang and Niki Trigoni and Andrew Markham},
   doi = {10.1109/TNNLS.2021.3112460},
   issn = {2162-237X},
   issue = {12},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep neural network (DNN),dynamical model,motion estimation,state-space model (SSM)},
   month = {12},
   pages = {5479-5491},
   pmid = {34559667},
   title = {DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction},
   volume = {32},
   url = {https://ieeexplore.ieee.org/document/9547669/},
   year = {2021},
}


@article{chen2021deep,
   abstract = {Inertial measurement units (IMUs) have emerged as an essential component in many of today's indoor navigation solutions due to their low cost and ease of use. However, despite many attempts for reducing the error growth of navigation systems based on commercial-grade inertial sensors, there is still no satisfactory solution that produces navigation estimates with long-time stability in widely differing conditions. This paper proposes to break the cycle of continuous integration used in traditional inertial algorithms, formulate it as an optimization problem, and explore the use of deep recurrent neural networks for estimating the displacement of a user over a specified time window. By training the deep neural network using inertial measurements and ground truth displacement data, it is possible to learn both motion characteristics and systematic error drift. As opposed to established context-aided inertial solutions, the proposed method is not dependent on either fixed sensor positions or periodic motion patterns. It can reconstruct accurate trajectories directly from raw inertial measurements, and predict the corresponding uncertainty to show model confidence. Extensive experimental evaluations demonstrate that the neural network produces position estimates with high accuracy for several different attachments, users, sensors, and motion types. As a particular demonstration of its flexibility, our deep inertial solutions can estimate trajectories for non-periodic motion, such as the shopping trolley tracking. Further more, it works in highly dynamic conditions, such as running, remaining extremely challenging for current techniques.},
   author = {Changhao Chen and Chris Xiaoxuan Lu and Johan Wahlstrom and Andrew Markham and Niki Trigoni},
   doi = {10.1109/TMC.2019.2960780},
   issn = {1536-1233},
   issue = {4},
   journal = {IEEE Transactions on Mobile Computing},
   keywords = {Pedestrian navigation,deep neural network,inertial indoor localization,inertial measurement units,learning from mobile sensor data},
   month = {4},
   pages = {1351-1364},
   publisher = {IEEE},
   title = {Deep Neural Network Based Inertial Odometry Using Low-Cost Inertial Measurement Units},
   volume = {20},
   url = {https://ieeexplore.ieee.org/document/8937008/},
   year = {2021},
}

@article{chen2020deep,
   abstract = {Modern inertial measurements units (IMUs) are small, cheap, energy efficient, and widely employed in smart devices and mobile robots. Exploiting inertial data for accurate and reliable pedestrian navigation supports is a key component for emerging Internet-of-Things applications and services. Recently, there has been a growing interest in applying deep neural networks (DNNs) to motion sensing and location estimation. However, the lack of sufficient labelled data for training and evaluating architecture benchmarks has limited the adoption of DNNs in IMU-based tasks. In this paper, we present and release the Oxford Inertial Odometry Dataset (OxIOD), a first-of-its-kind public dataset for deep learning based inertial navigation research, with fine-grained ground-truth on all sequences. Furthermore, to enable more efficient inference at the edge, we propose a novel lightweight framework to learn and reconstruct pedestrian trajectories from raw IMU data. Extensive experiments show the effectiveness of our dataset and methods in achieving accurate data-driven pedestrian inertial navigation on resource-constrained devices.},
   author = {Changhao Chen and Peijun Zhao and Chris Xiaoxuan Lu and Wei Wang and Andrew Markham and Niki Trigoni},
   doi = {10.1109/JIOT.2020.2966773},
   issn = {2327-4662},
   issue = {5},
   journal = {IEEE Internet of Things Journal},
   month = {5},
   pages = {4431-4441},
   title = {Deep-Learning-Based Pedestrian Inertial Navigation: Methods, Data Set, and On-Device Inference},
   volume = {7},
   url = {https://ieeexplore.ieee.org/document/8960327/},
   year = {2020},
}

@article{chen2018oxiod,
   abstract = {Advances in micro-electro-mechanical (MEMS) techniques enable inertial measurements units (IMUs) to be small, cheap, energy efficient, and widely used in smartphones, robots, and drones. Exploiting inertial data for accurate and reliable navigation and localization has attracted significant research and industrial interest, as IMU measurements are completely ego-centric and generally environment agnostic. Recent studies have shown that the notorious issue of drift can be significantly alleviated by using deep neural networks (DNNs), e.g. IONet. However, the lack of sufficient labelled data for training and testing various architectures limits the proliferation of adopting DNNs in IMU-based tasks. In this paper, we propose and release the Oxford Inertial Odometry Dataset (OxIOD), a first-of-its-kind data collection for inertial-odometry research, with all sequences having ground-truth labels. Our dataset contains 158 sequences totalling more than 42 km in total distance, much larger than previous inertial datasets. Another notable feature of this dataset lies in its diversity, which can reflect the complex motions of phone-based IMUs in various everyday usage. The measurements were collected with four different attachments (handheld, in the pocket, in the handbag and on the trolley), four motion modes (halting, walking slowly, walking normally, and running), five different users, four types of off-the-shelf consumer phones, and large-scale localization from office buildings. Deep inertial tracking experiments were conducted to show the effectiveness of our dataset in training deep neural network models and evaluate learning-based and model-based algorithms. The OxIOD Dataset is available at: http://deepio.cs.ox.ac.uk},
   author = {Changhao Chen and Peijun Zhao and Chris Xiaoxuan Lu and Wei Wang and Andrew Markham and Niki Trigoni},
   month = {9},
   title = {OxIOD: The Dataset for Deep Inertial Odometry},
   url = {http://arxiv.org/abs/1809.07491},
   year = {2018},
}

@article{chen2018transferring,
   abstract = {Inertial information processing plays a pivotal role in ego-motion awareness for mobile agents, as inertial measurements are entirely egocentric and not environment dependent. However, they are affected greatly by changes in sensor placement/orientation or motion dynamics, and it is infeasible to collect labelled data from every domain. To overcome the challenges of domain adaptation on long sensory sequences, we propose a novel framework that extracts domain-invariant features of raw sequences from arbitrary domains, and transforms to new domains without any paired data. Through the experiments, we demonstrate that it is able to efficiently and effectively convert the raw sequence from a new unlabelled target domain into an accurate inertial trajectory, benefiting from the physical motion knowledge transferred from the labelled source domain. We also conduct real-world experiments to show our framework can reconstruct physically meaningful trajectories from raw IMU measurements obtained with a standard mobile phone in various attachments.},
   author = {Changhao Chen and Yishu Miao and Chris Xiaoxuan Lu and Phil Blunsom and Andrew Markham and Niki Trigoni},
   month = {10},
   title = {Transferring Physical Motion Between Domains for Neural Inertial Tracking},
   url = {http://arxiv.org/abs/1810.02076},
   year = {2018},
}

@article{chen2018ionet,
   abstract = {Inertial sensors play a pivotal role in indoor localization, which in turn lays the foundation for pervasive personal applications. However, low-cost inertial sensors, as commonly found in smartphones, are plagued by bias and noise, which leads to unbounded growth in error when accelerations are double integrated to obtain displacement. Small errors in state estimation propagate to make odometry virtually unusable in a matter of seconds. We propose to break the cycle of continuous integration, and instead segment inertial data into independent windows. The challenge becomes estimating the latent states of each window, such as velocity and orientation, as these are not directly observable from sensor data. We demonstrate how to formulate this as an optimization problem, and show how deep recurrent neural networks can yield highly accurate trajectories, outperforming state-of-the-art shallow techniques, on a wide range of tests and attachments. In particular, we demonstrate that IONet can generalize to estimate odometry for non-periodic motion, such as a shopping trolley or baby-stroller, an extremely challenging task for existing techniques.},
   author = {Changhao Chen and Xiaoxuan Lu and Andrew Markham and Niki Trigoni},
   doi = {10.1609/aaai.v32i1.12102},
   issn = {2374-3468},
   issue = {1},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {4},
   pages = {6468-6476},
   title = {IONet: Learning to Cure the Curse of Drift in Inertial Odometry},
   volume = {32},
   url = {https://ojs.aaai.org/index.php/AAAI/article/view/12102},
   year = {2018},
}

@article{Tang2022,
   abstract = {In filtering-based global navigation satellite system (GNSS) / inertial navigation system (INS) integrated navigation systems, the precise INS mechanization has fully considered the impact of the Earth rotation, Coriolis acceleration, etc. However, most inertial measurement unit (IMU) preintegration models in factor graph optimization (FGO)-based integrated navigation systems are rough and ignore these factors. We propose an FGO-based GNSS/INS integrated navigation system to analyze and evaluate the impact of the Earth rotation on MEMS-IMU preintegration. The proposed FGO-based integration is based on a refined IMU preintegration, in which the Earth rotation is compensated. The proposed GNSS/INS integration is a sliding-window optimizer in which the GNSS positioning and the IMU preintegration are tightly fused within the FGO framework. The simulated GNSS outage in field tests, an effective method, is adopted to evaluate the IMU preintegration quantitatively. The results demonstrate that the refined IMU preintegration can achieve the same accuracy as the precise INS mechanization. In contrast, the rough IMU preintegration and INS mechanization without compensating for the Earth rotation yields notable accuracy degradation. When the GNSS-outage time is 60 seconds, the degradation can be 200% for the industrial-grade MEMS module and more than 10% for the consumer-grade MEMS chip. Besides, the degradation can be more significant if the GNSS-outage time is longer. The proposed FGO-based GNSS/INS integration (https://github.com/i2Nav-WHU/OB_GINS) and the employed datasets (https://github.com/i2Nav-WHU/awesome-gins-datasets) are open-sourced on GitHub.},
   author = {Hailiang Tang and Tisheng Zhang and Xiaoji Niu and Jing Fan and Jingnan Liu},
   doi = {10.1109/JSEN.2022.3192552},
   issn = {15581748},
   issue = {17},
   journal = {IEEE Sensors Journal},
   keywords = {GNSS/INS integration,MEMS-IMU preintegration,factor graph optimization},
   month = {9},
   pages = {17194-17204},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Impact of the Earth Rotation Compensation on MEMS-IMU Preintegration of Factor Graph Optimization},
   volume = {22},
   year = {2022},
}

% https://blog.csdn.net/chenglinjuan/article/details/105667166 参考文献里面的&字符需要做转义处理
@inproceedings{herath2020ronin,
   abstract = {This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of horizontal positions and heading direction of a moving subject from a sequence of IMU sensor measurements from a phone. In contrast to existing methods, our method can handle varying phone orientations and placements.More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We share the code and data to promote further research. (http://ronin.cs.sfu.ca).},
   author = {Sachini Herath and Hang Yan and Yasutaka Furukawa},
   doi = {10.1109/ICRA40945.2020.9196860},
   isbn = {9781728173955},
   issn = {10504729},
   journal = {IEEE International Conference on Robotics and Automation (ICRA)},
   pages = {3146-3152},
   title = {RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, \& New Methods},
   year = {2020},
}

@article{Geiger2013IJRR,
   abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10–100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
   author = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun},
   doi = {10.1177/0278364913491297},
   issn = {0278-3649},
   issue = {11},
   journal = {The International Journal of Robotics Research},
   keywords = {Dataset,GPS,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
   month = {9},
   pages = {1231-1237},
   title = {Vision meets robotics: The KITTI dataset},
   volume = {32},
   url = {http://journals.sagepub.com/doi/10.1177/0278364913491297},
   year = {2013},
}